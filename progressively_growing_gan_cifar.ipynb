{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_folder = 'resized_cifar/'\n",
    "X_train = {}\n",
    "for res in [4,8,16,32]:\n",
    "    data_file_prefix = data_file_folder + \"size_\" + str(res) + \"_batch_\"\n",
    "    files = [data_file_prefix + str(i) for i in range(6)]\n",
    "    X_train[res] = np.concatenate([ unpickle(data_file)['data'] for data_file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 4, 4, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_and_bn(X, bn_name, alpha=0.3):\n",
    "    X = tf.layers.batch_normalization(X, name=bn_name)\n",
    "    return tf.nn.relu(X) - alpha * tf.nn.relu(-X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "# -----\n",
    "# Phase: 0 (initial)\n",
    "# Input: 3 x 4 x 4\n",
    "\n",
    "# Block: from_rgb_4\n",
    "# c1 (from_rbg): 128 x 4 x 4\n",
    "\n",
    "# Block: out\n",
    "# c3: 128 x 4 x 4\n",
    "# c4: 128 x 1 x 1\n",
    "# fc: 1 x 1 x 1\n",
    "\n",
    "# -----\n",
    "# Phase: 1 (add 1)\n",
    "# Input: 3 x 8 x 8\n",
    "\n",
    "# Block: from_rgb_8\n",
    "# c1 (from_rbg): 64 x 8 x 8\n",
    "\n",
    "# Block: added_8_to_4\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 128 x 8 x 8\n",
    "# dn: 128 x 4 x 4\n",
    "\n",
    "# Block: out\n",
    "# c3: 128 x 4 x 4\n",
    "# c4: 128 x 1 x 1\n",
    "# fc: 1 x 1 x 1\n",
    "\n",
    "# -----\n",
    "# Phase: 2 (add 2)\n",
    "# Input: 3 x 16 x 16\n",
    "\n",
    "# Block: from_rgb_16\n",
    "# c1 (from_rbg): 32 x 16 x 16\n",
    "\n",
    "# Block: added_16_to_8\n",
    "# c3: 32 x 16 x 16\n",
    "# c3: 64 x 16 x 16\n",
    "# dn: 64 x 8 x 8\n",
    "\n",
    "# Block: added_8_to_4\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 128 x 8 x 8\n",
    "# dn: 128 x 4 x 4\n",
    "\n",
    "# Block: out\n",
    "# c3: 128 x 4 x 4\n",
    "# c4: 128 x 1 x 1\n",
    "# fc: 1 x 1 x 1\n",
    "\n",
    "# -----\n",
    "# Phase: 3 (add 3)\n",
    "# Input: 3 x 32 x 32\n",
    "\n",
    "# Block: from_rgb_32\n",
    "# c1 (from_rbg): 16 x 32 x 32\n",
    "\n",
    "# Block: added_32_to_16\n",
    "# c3: 16 x 32 x 32\n",
    "# c3: 32 x 32 x 32\n",
    "# dn: 32 x 16 x 16\n",
    "\n",
    "# Block: added_16_to_8\n",
    "# c3: 32 x 16 x 16\n",
    "# c3: 64 x 16 x 16\n",
    "# dn: 64 x 8 x 8\n",
    "\n",
    "# Block: added_8_to_4\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 128 x 8 x 8\n",
    "# dn: 128 x 4 x 4\n",
    "\n",
    "# Block: out\n",
    "# c3: 128 x 4 x 4\n",
    "# c4: 128 x 1 x 1\n",
    "# fc: 1 x 1 x 1\n",
    "\n",
    "# ><><><><><><><><><><><><\n",
    "\n",
    "# Generator\n",
    "# ------\n",
    "# Phase: 0 (Initial)\n",
    "# Input z: 128 x 1 x 1\n",
    "\n",
    "# Block: initial\n",
    "# c4: 128 x 4 x 4\n",
    "# c3: 128 x 4 x 4\n",
    "\n",
    "# Block: to_rgb_4\n",
    "# c1 (to_rgb): 3 x 4 x 4\n",
    "\n",
    "# ----\n",
    "# Phase: 1 (add 1)\n",
    "# Input z: 128 x 1 x 1\n",
    "\n",
    "# Block: initial\n",
    "# c4: 128 x 4 x 4\n",
    "# c3: 128 x 4 x 4\n",
    "\n",
    "# Block: added_4_to_8\n",
    "# up: 128 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "\n",
    "# Block: to_rgb_8\n",
    "# c1 (to_rgb): 3 x 8 x 8\n",
    "\n",
    "# -----\n",
    "# Phase: 2 (add 2)\n",
    "# Input z: 128 x 1 x 1\n",
    "\n",
    "# Block: initial\n",
    "# c4: 128 x 4 x 4\n",
    "# c3: 128 x 4 x 4\n",
    "\n",
    "# Block: added_4_to_8\n",
    "# up: 128 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "\n",
    "# Block: added_8_to_16\n",
    "# up: 64 x 16 x 16\n",
    "# c3: 32 x 16 x 16\n",
    "# c3: 32 x 16 x 16\n",
    "\n",
    "# Block: to_rgb_16\n",
    "# c1 (to_rgb): 3 x 16 x 16\n",
    "\n",
    "# ----\n",
    "# Phase: 3 (add 3)\n",
    "# Input z: 128 x 1 x 1\n",
    "\n",
    "# Block: initial\n",
    "# c4: 128 x 4 x 4\n",
    "# c3: 128 x 4 x 4\n",
    "\n",
    "# Block: added_4_to_8\n",
    "# up: 128 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "# c3: 64 x 8 x 8\n",
    "\n",
    "# Block: added_8_to_16\n",
    "# up: 64 x 16 x 16\n",
    "# c3: 32 x 16 x 16\n",
    "# c3: 32 x 16 x 16\n",
    "\n",
    "# Block: added_16_to_32\n",
    "# up: 32 x 32 x 32\n",
    "# c3: 16 x 32 x 32\n",
    "# c3: 16 x 32 x 32\n",
    "\n",
    "# Block: to_rgb_32\n",
    "# c1 (to_rgb): 3 x 32 x 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config:\n",
    "batch_size = 64\n",
    "num_images_before_adding_block_constant = 1600000\n",
    "d_warm_up = 0\n",
    "verbose = True\n",
    "print_frequency = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_blocks_to_add_and_smoothing_coefficient(num_images_seen):\n",
    "    num_images_before_adding_block = tf.constant(num_images_before_adding_block_constant, tf.float32)\n",
    "\n",
    "    num_blocks_to_add = tf.floor(num_images_seen / num_images_before_adding_block)\n",
    "    num_blocks_to_add = tf.cond(num_blocks_to_add > 3, lambda: tf.minimum(num_blocks_to_add, tf.constant(3.0)), lambda: num_blocks_to_add)\n",
    "\n",
    "    num_images_into_phase = num_images_seen - (num_blocks_to_add * num_images_before_adding_block)  \n",
    "    smoothing_coefficient = tf.cond(num_blocks_to_add >= 3,\n",
    "        lambda: tf.constant(1.0),\n",
    "        lambda: tf.maximum(tf.constant(1.0), num_images_into_phase / (num_images_before_adding_block / 2)))\n",
    "    \n",
    "    # Don't smooth in starting resolution since there is no prior\n",
    "    smoothing_coefficient = tf.cond(tf.equal(num_blocks_to_add, 0), lambda: tf.constant(1.0), lambda: smoothing_coefficient)\n",
    "    \n",
    "    return [num_blocks_to_add, smoothing_coefficient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_and_bn(X, bn_name, alpha=0.3):\n",
    "    X = tf.layers.batch_normalization(X, name=bn_name)\n",
    "    return tf.nn.relu(X) - alpha * tf.nn.relu(-X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, resolution, resolution, 3\n",
    "# Out: batch_size, resolution, resolution, target_num_channels \n",
    "def from_rgb(X, resolution, target_num_channels, reuse=None):\n",
    "    with tf.variable_scope(\"from_rgb_\" + str(resolution)):\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        X_normalized = tf.layers.batch_normalization(X, name='bn0' + str(resolution))\n",
    "        w = tf.get_variable('w' + str(resolution), [1, 1, 3, target_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / target_num_channels)))\n",
    "        b = tf.get_variable('b' + str(resolution), [target_num_channels], initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        conv = tf.nn.conv2d(X_normalized, w, [1,1,1,1], padding='SAME') + b\n",
    "        return leaky_relu_and_bn(conv, 'bn1' + str(resolution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 4*2^phase, 4*2^phase, 3\n",
    "# Out: batch_size, 4*2^phase, 4*2^phase, 128/(2^phase)\n",
    "def from_rgb_for_phase(X, phase, reuse=None):\n",
    "    return tf.case([\n",
    "            (tf.equal(phase, tf.constant(0.0)), lambda: from_rgb(X, 4, 128, reuse)),\n",
    "            (tf.equal(phase, tf.constant(1.0)), lambda: from_rgb(X, 8, 64, reuse)),\n",
    "            (tf.equal(phase, tf.constant(2.0)), lambda: from_rgb(X, 16, 32, reuse)),\n",
    "            (tf.equal(phase, tf.constant(3.0)), lambda: from_rgb(X, 32, 16, reuse)),\n",
    "        ], default=lambda: tf.zeros((1,4,4,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 4*2^phase, 4*2^phase, 3\n",
    "# Out: batch_size, 4*2^phase, 4*2^phase, 128/(2^phase) \n",
    "def from_rgb_possibly_smoothed(X, phase, smoothing_coefficient):\n",
    "    from_rgb = from_rgb_for_phase(X, phase)\n",
    "    \n",
    "    def smoothed():\n",
    "        half_resolution_X = tf.nn.avg_pool(X, [1, 2, 2, 1], [1, 1, 1, 1], padding='SAME')\n",
    "        prior_from_rgb = from_rgb_for_phase(half_resolution_X, phase - 1, True)\n",
    "        return (1 - smoothing_coefficient) * prior_from_rgb + smoothing_coefficient * from_rgb\n",
    "    \n",
    "    return tf.cond(smoothing_coefficient < 1.0, smoothed, lambda: from_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, input_resolution, input_resolution, input_num_channels\n",
    "# Out: batch_size, input_resolution / 2, input_resolution / 2, 2 * input_num_channels\n",
    "def added_discriminator_block(X, input_resolution, input_num_channels):\n",
    "    target_resolution = input_resolution / 2\n",
    "    target_num_channels = 2 * input_num_channels\n",
    "    with tf.variable_scope(\"added_\" + str(input_resolution) + \"_to_\" + str(target_resolution)):\n",
    "        w1 = tf.get_variable('w1', [3, 3, input_num_channels, input_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / input_num_channels)))\n",
    "        b1 = tf.get_variable('b1', [input_num_channels], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        conv1 = tf.nn.conv2d(X, w1, [1,1,1,1], padding='SAME', name='conv1') + b1\n",
    "        a1 = leaky_relu_and_bn(conv1, 'bn1')\n",
    "\n",
    "        w2 = tf.get_variable('w2', [3, 3, input_num_channels, target_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / input_num_channels)))\n",
    "        b2 = tf.get_variable('b2', [target_num_channels], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        conv2 = tf.nn.conv2d(a1, w2, [1,1,1,1], padding='SAME', name='conv2') + b2\n",
    "        a2 = leaky_relu_and_bn(conv2, 'bn2')  \n",
    "\n",
    "        return tf.nn.avg_pool(a2, [1, 2, 2, 1], [1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 4, 4, 128\n",
    "# Out: batch_size, 1, 1, 1\n",
    "def discriminator_output_block(X):\n",
    "    with tf.variable_scope(\"output_block\"):\n",
    "        w1 = tf.get_variable('w1', [3, 3, 128, 128], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / 128)))\n",
    "        b1 = tf.get_variable('b1', [128], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        conv1 = tf.nn.conv2d(X, w1, [1,1,1,1], padding='SAME') + b1\n",
    "        a1 = leaky_relu_and_bn(conv1, 'bn1')\n",
    "\n",
    "        w2 = tf.get_variable('w2', [4, 4, 128, 128], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / 128)))\n",
    "        b2 = tf.get_variable('b2', [128], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        conv2 = tf.nn.conv2d(a1, w2, [1,1,1,1], padding='VALID') + b2\n",
    "        a2 = leaky_relu_and_bn(conv2, 'bn2')\n",
    "        return tf.layers.dense(a2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 4*2^phase, 4*2^phase, 3\n",
    "# Out: batch_size, 1, 1, 1\n",
    "def discriminator(X, num_images_seen, reuse=False):\n",
    "    with tf.variable_scope(\"D\"):\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        num_blocks_to_add, smoothing_coefficient = calculate_num_blocks_to_add_and_smoothing_coefficient(num_images_seen)\n",
    "        phase = num_blocks_to_add\n",
    "\n",
    "        X = from_rgb_possibly_smoothed(X, phase, smoothing_coefficient)\n",
    "        \n",
    "        X = tf.cond(phase >= 3, lambda: added_discriminator_block(X, 32, 16), lambda: X)\n",
    "        X = tf.cond(phase >= 2, lambda: added_discriminator_block(X, 16, 32), lambda: X)\n",
    "        X = tf.cond(phase >= 1, lambda: added_discriminator_block(X, 8, 64), lambda: X)\n",
    "\n",
    "        return discriminator_output_block(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 128, 1, 1\n",
    "# Out: batch_size, 128, 4, 4\n",
    "def initial_generator_block(z):\n",
    "    with tf.variable_scope(\"initial_block\"):\n",
    "        w1 = tf.get_variable('w1', [4, 4, 128, 128], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / 128)))\n",
    "        b1 = tf.get_variable('b1', [128], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        conv1 = tf.nn.conv2d_transpose(z, w1, [batch_size, 4, 4, 128], [1, 1, 1, 1], padding='VALID', name='conv1') + b1      \n",
    "        a1 = leaky_relu_and_bn(conv1, 'bn1')\n",
    "\n",
    "        w2 = tf.get_variable('w2', [3, 3, 128, 128], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / 128)))\n",
    "        b2 = tf.get_variable('b2', [128], initializer = tf.truncated_normal_initializer(stddev=0.02))    \n",
    "        a2 = tf.nn.conv2d_transpose(a1, w2, [batch_size, 4, 4, 128], [1, 1, 1, 1], padding='SAME', name='conv2') + b2\n",
    "        return leaky_relu_and_bn(a2, 'bn2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, starting_resolution, starting_resolution, starting_num_channels\n",
    "# Out: [\n",
    "#       (batch_size, 2 * starting_resolution, 2 * starting_resolution, starting_num_channels), # result after doubling\n",
    "#       (batch_size, 2 * starting_resolution, 2 * starting_resolution, starting_num_channels / 2) # block output at target resolution\n",
    "# ]\n",
    "def added_generator_block(X, starting_resolution, starting_num_channels):\n",
    "    target_resolution = 2 * starting_resolution\n",
    "    target_num_channels = starting_num_channels / 2\n",
    "    with tf.variable_scope(\"added_\" + str(starting_resolution) + \"_to_\" + str(target_resolution)):\n",
    "        double_resolution = tf.image.resize_nearest_neighbor(X, [target_resolution, target_resolution])\n",
    "        w1 = tf.get_variable('w1', [3, 3, target_num_channels, starting_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / starting_num_channels)))\n",
    "        b1 = tf.get_variable('b1', [target_num_channels], initializer = tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / target_num_channels)))\n",
    "        deconv1 = tf.nn.conv2d_transpose(double_resolution, w1, [batch_size, target_resolution, target_resolution, target_num_channels], [1,1,1,1], padding='SAME', name='deconv1') + b1\n",
    "        a1 = leaky_relu_and_bn(deconv1, 'bn1')\n",
    "\n",
    "        w2 = tf.get_variable('w2', [3, 3, target_num_channels, target_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / target_num_channels)))\n",
    "        b2 = tf.get_variable('b2', [target_num_channels], initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        deconv2 = tf.nn.conv2d_transpose(a1, w2, [batch_size, target_resolution, target_resolution, target_num_channels], [1,1,1,1], padding='SAME', name='deconv2') + b2\n",
    "        return [double_resolution, leaky_relu_and_bn(deconv2, 'bn2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, resolution, resolution, input_num_channels\n",
    "# Out: batch_size, resolution, resolution, 3\n",
    "def to_rgb(X, resolution, input_num_channels, reuse=None):\n",
    "    with tf.variable_scope(\"to_rgb_\" + str(resolution)):\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        w = tf.get_variable('w', [1, 1, 3, input_num_channels], initializer=tf.truncated_normal_initializer(stddev=np.sqrt(2.0 / input_num_channels)))\n",
    "        b = tf.get_variable('b', [3], initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        return tf.nn.conv2d_transpose(X, w, [batch_size, resolution, resolution, 3], [1,1,1,1], padding='SAME', name='conv') + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_for_phase(X, phase, reuse=None):\n",
    "    return tf.case([\n",
    "        (tf.equal(phase, tf.constant(0.0)), lambda: to_rgb(X, 4, 128, reuse)),\n",
    "        (tf.equal(phase, tf.constant(1.0)), lambda: to_rgb(X, 8, 64, reuse)),\n",
    "        (tf.equal(phase, tf.constant(2.0)), lambda: to_rgb(X, 16, 32, reuse)),\n",
    "        (tf.equal(phase, tf.constant(3.0)), lambda: to_rgb(X, 32, 16, reuse)),\n",
    "    ], default=lambda: tf.zeros((1,4,4,128), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_possibly_smoothed(X, doubled_prior, phase, smoothing_coefficient):\n",
    "    new_to_rgb = to_rgb_for_phase(X, phase) \n",
    "    return tf.cond(smoothing_coefficient < 1,\n",
    "        lambda: (1 - smoothing_coefficient) * to_rgb_for_phase(doubled_prior, phase, True)  + smoothing_coefficient * new_to_rgb,\n",
    "        lambda: new_to_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_output_for_phase(X, phase):\n",
    "    # this disaster of code style is because tf.case doesn't preserve shape\n",
    "    return tf.cond(tf.equal(phase, tf.constant(0.0)), lambda: tf.reshape(X, (batch_size, 4, 4, 3)),\n",
    "        lambda: tf.cond(tf.equal(phase, tf.constant(1.0)), lambda: tf.reshape(X, (batch_size, 8, 8, 3)),\n",
    "            lambda: tf.cond(tf.equal(phase, tf.constant(2.0)), lambda: tf.reshape(X, (batch_size, 16, 16, 3)),\n",
    "                lambda: tf.reshape(X, (batch_size, 32, 32, 3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In: batch_size, 128\n",
    "# Out: batch_size, 4*2^phase, 4*2^phase, 3\n",
    "def generator(z, num_images_seen, reuse=False):\n",
    "    with tf.variable_scope(\"G\"):\n",
    "        if (reuse):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        num_blocks_to_add, smoothing_coefficient = calculate_num_blocks_to_add_and_smoothing_coefficient(num_images_seen)\n",
    "        phase = num_blocks_to_add\n",
    "        z_reshaped = tf.reshape(z, [batch_size, 1, 1, 128], name='z_reshape')\n",
    "        \n",
    "        X = initial_generator_block(z_reshaped)\n",
    "        \n",
    "        doubled_prior, X = tf.cond(phase >= 1, lambda: added_generator_block(X, 4, 128), lambda: [X, X])\n",
    "        doubled_prior, X = tf.cond(phase >= 2, lambda: added_generator_block(X, 8, 64), lambda: [doubled_prior, X])\n",
    "        doubled_prior, X = tf.cond(phase >= 3, lambda: added_generator_block(X, 16, 32), lambda: [doubled_prior, X])\n",
    "\n",
    "        X = to_rgb_possibly_smoothed(X, doubled_prior, phase, smoothing_coefficient)\n",
    "        return reshape_output_for_phase(X, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [batch_size, None, None, 3], name=\"X\")\n",
    "z = tf.placeholder(tf.float32, [batch_size, 128], name=\"z\")\n",
    "num_images_seen = tf.placeholder(tf.float32, name='num_images_seen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Dx = discriminator(X, num_images_seen)\n",
    "Gz = generator(z, num_images_seen, False)\n",
    "Dg = discriminator(Gz, num_images_seen, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cost_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.ones_like(Dx)))\n",
    "d_cost_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.zeros_like(Dg)))\n",
    "g_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.ones_like(Dg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if 'D/' in var.name]\n",
    "g_vars = [var for var in t_vars if 'G/' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_d_real = tf.train.AdamOptimizer(learning_rate=0.001).minimize(d_cost_real, var_list=d_vars)\n",
    "optimizer_d_fake = tf.train.AdamOptimizer(learning_rate=0.001).minimize(d_cost_fake, var_list=d_vars)\n",
    "optimizer_g = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_cost, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_params(varbs):\n",
    "    total_parameters = 0\n",
    "    for variable in varbs:\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# D params:\n",
      "558121\n",
      "# G params:\n",
      "556924\n",
      "Total # params:\n",
      "1115045\n"
     ]
    }
   ],
   "source": [
    "print(\"# D params:\")\n",
    "print_num_params(d_vars)\n",
    "print(\"# G params:\")\n",
    "print_num_params(g_vars)\n",
    "print(\"Total # params:\")\n",
    "print_num_params(t_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New image size: 4\n",
      "New num_block_to_add: 0\n",
      "# images seen: 64\n",
      "Cost Real: 131.773\n",
      "Cost Fake: 0.640741\n",
      "g_cost:9.3875\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 60032\n",
      "Cost Real: 14.79\n",
      "Cost Fake: 6.65889\n",
      "g_cost:55.7958\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 120000\n",
      "Cost Real: 89.0421\n",
      "Cost Fake: 0.0\n",
      "g_cost:217.914\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 179968\n",
      "Cost Real: 44.9338\n",
      "Cost Fake: 0.402007\n",
      "g_cost:28.6059\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 239936\n",
      "Cost Real: 1.61781\n",
      "Cost Fake: 0.185992\n",
      "g_cost:12.915\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 299904\n",
      "Cost Real: 1.4287\n",
      "Cost Fake: 0.194268\n",
      "g_cost:2.60309\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 359872\n",
      "Cost Real: 1.78794\n",
      "Cost Fake: 0.291603\n",
      "g_cost:2.65465\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 419840\n",
      "Cost Real: 1.3263\n",
      "Cost Fake: 0.289667\n",
      "g_cost:1.27352\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 479808\n",
      "Cost Real: 1.49563\n",
      "Cost Fake: 0.328893\n",
      "g_cost:0.865217\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 539776\n",
      "Cost Real: 9.77692\n",
      "Cost Fake: 0.000188867\n",
      "g_cost:12.2442\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 599744\n",
      "Cost Real: 0.191353\n",
      "Cost Fake: 3.85296\n",
      "g_cost:0.758943\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 659712\n",
      "Cost Real: 17.2707\n",
      "Cost Fake: 9.91451\n",
      "g_cost:0.000160784\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 719680\n",
      "Cost Real: 151.664\n",
      "Cost Fake: 0.0\n",
      "g_cost:92.8818\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 779648\n",
      "Cost Real: 57.6902\n",
      "Cost Fake: 1.5235e-10\n",
      "g_cost:43.9716\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 839616\n",
      "Cost Real: 19.5631\n",
      "Cost Fake: 15.9779\n",
      "g_cost:18.9186\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 899584\n",
      "Cost Real: 77.9929\n",
      "Cost Fake: 2.18108e-26\n",
      "g_cost:69.7105\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 959552\n",
      "Cost Real: 25.9927\n",
      "Cost Fake: 1.32768\n",
      "g_cost:53.1694\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1019520\n",
      "Cost Real: 23.7509\n",
      "Cost Fake: 9.34305e-07\n",
      "g_cost:18.7383\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1079488\n",
      "Cost Real: 0.0\n",
      "Cost Fake: 135.845\n",
      "g_cost:0.0\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1139456\n",
      "Cost Real: 11.8239\n",
      "Cost Fake: 7.98938e-05\n",
      "g_cost:0.0\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1199424\n",
      "Cost Real: 85.162\n",
      "Cost Fake: 1.23373e-09\n",
      "g_cost:17.6696\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1259392\n",
      "Cost Real: 2.25325\n",
      "Cost Fake: 9.57846\n",
      "g_cost:16.4\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1319360\n",
      "Cost Real: 63.9431\n",
      "Cost Fake: 1.05457e-10\n",
      "g_cost:82.4049\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1379328\n",
      "Cost Real: 9.16635\n",
      "Cost Fake: 0.000487685\n",
      "g_cost:1.50157\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1439296\n",
      "Cost Real: 79.4503\n",
      "Cost Fake: 1.44029e-33\n",
      "g_cost:106.448\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1499264\n",
      "Cost Real: 0.000221206\n",
      "Cost Fake: 57.7949\n",
      "g_cost:1.62074e-16\n",
      "num to add: 0\n",
      "image size: 4\n",
      "# images seen: 1559232\n",
      "Cost Real: 51.976\n",
      "Cost Fake: 4.2243\n",
      "g_cost:1.88011\n",
      "num to add: 0\n",
      "image size: 4\n",
      "New image size: 8\n",
      "New num_block_to_add: 1\n",
      "# images seen: 1619200\n",
      "Cost Real: 3725.13\n",
      "Cost Fake: 665391.0\n",
      "g_cost:0.0\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1679168\n",
      "Cost Real: 42741.1\n",
      "Cost Fake: 101936.0\n",
      "g_cost:2117.76\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1739136\n",
      "Cost Real: 4529.76\n",
      "Cost Fake: 53329.9\n",
      "g_cost:0.0\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1799104\n",
      "Cost Real: 2076.66\n",
      "Cost Fake: 2372.09\n",
      "g_cost:0.0\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1859072\n",
      "Cost Real: 43677.2\n",
      "Cost Fake: 0.0\n",
      "g_cost:23813.5\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1919040\n",
      "Cost Real: 0.265412\n",
      "Cost Fake: 30205.9\n",
      "g_cost:3563.15\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 1979008\n",
      "Cost Real: 0.0\n",
      "Cost Fake: 93476.1\n",
      "g_cost:0.0\n",
      "num to add: 1\n",
      "image size: 8\n",
      "# images seen: 2038976\n",
      "Cost Real: 252390.0\n",
      "Cost Fake: 57370.8\n",
      "g_cost:830787.0\n",
      "num to add: 1\n",
      "image size: 8\n"
     ]
    }
   ],
   "source": [
    "sample = None\n",
    "with tf.Session() as session:\n",
    "    tf.summary.scalar('Generator_loss', g_cost)\n",
    "    tf.summary.scalar('Discriminator_loss_real', d_cost_real)\n",
    "    tf.summary.scalar('Discriminator_loss_fake', d_cost_fake)\n",
    "\n",
    "    images_for_tensorboard = generator(z, num_images_seen, True)\n",
    "    tf.summary.image('Generated_images', images_for_tensorboard, 32)\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, session.graph)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    num_training_images = X_train[4].shape[0]\n",
    "    num_images_seen_int = 0\n",
    "    prior_image_size = -1\n",
    "    prior_num_blocks_to_add = -1\n",
    "    while True:\n",
    "        for i in range(0, (int) (math.floor(X_train[4].shape[0] / batch_size) * batch_size), batch_size):\n",
    "            num_blocks_to_add = int(min(math.floor(num_images_seen_int / num_images_before_adding_block_constant), 3))\n",
    "            image_size = 2 ** (2 + num_blocks_to_add)\n",
    "            \n",
    "            if prior_image_size != image_size:\n",
    "                print(\"New image size: \" + str(image_size))\n",
    "                        \n",
    "            if prior_num_blocks_to_add != num_blocks_to_add:\n",
    "                print(\"New num_block_to_add: \" + str(num_blocks_to_add))\n",
    "            \n",
    "            original_real_batch = X_train[image_size][i:i + batch_size]\n",
    "            real_batch = map(lambda(im): im.reshape(image_size, image_size, 3, order='F'), original_real_batch)\n",
    "            z_batch = np.random.normal(0, 1, size=[batch_size, 128])\n",
    "            _, __, cost_real, cost_fake = session.run([optimizer_d_real, optimizer_d_fake, d_cost_real, d_cost_fake], feed_dict={X: real_batch, z: z_batch, num_images_seen: num_images_seen_int })\n",
    "            \n",
    "            num_images_seen_int += len(real_batch)\n",
    "            if num_images_seen_int >= d_warm_up:\n",
    "                _, gen_cost = session.run([optimizer_g, g_cost], feed_dict={z: z_batch, num_images_seen: num_images_seen_int})\n",
    "                if verbose and i % print_frequency == 0:\n",
    "                    print \"# images seen: \" + str(num_images_seen_int)\n",
    "                    print \"Cost Real: \" + str(cost_real)\n",
    "                    print \"Cost Fake: \" + str(cost_fake)\n",
    "                    print \"g_cost:\" + str(gen_cost)\n",
    "                    print(\"num to add: \" + str(num_blocks_to_add))\n",
    "                    print(\"image size: \") + str(image_size)\n",
    "\n",
    "                    generated_images = generator(z, num_images_seen, True)\n",
    "                    images = session.run(generated_images, {z: z_batch, num_images_seen: num_images_seen_int})\n",
    "                    \n",
    "                    # Update TensorBoard with summary statistics\n",
    "                    summary = session.run(merged, {z: z_batch, X: real_batch, num_images_seen: num_images_seen_int})\n",
    "                    writer.add_summary(summary, num_images_seen_int)\n",
    "                    \n",
    "                    sample = images[0]\n",
    "\n",
    "            prior_num_blocks_to_add = num_blocks_to_add\n",
    "            prior_image_size = image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
